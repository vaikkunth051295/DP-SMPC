{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspired from Andrew Ng's course on Machine Learning\n",
    "def logisticRegression():\n",
    "    \n",
    "    data = pd.read_csv(\"logReg.csv\")\n",
    "    \n",
    "    X=data.iloc[:,:-1].values\n",
    "    y=data.iloc[:,-1].values\n",
    "\n",
    "    # Feature mapping \n",
    "    def featureMap(x_1,x_2,deg): # Generating All plausible Polynomial terms upto degree 'deg'\n",
    "\n",
    "        res = np.ones(len(x_1)).reshape(len(x_1),1)\n",
    "        for i in range(1,deg+1):\n",
    "            for j in range(i+1):\n",
    "                terms= (x_1**(i-j) * x_2**j).reshape(len(x_1),1)\n",
    "                res= np.hstack((res,terms))\n",
    "        return res\n",
    "\n",
    "    X = featureMap(X[:,0], X[:,1],6)\n",
    "    \n",
    "    def sigmoid(k):\n",
    "        z=(1 + np.exp(-k))\n",
    "        return 1/z \n",
    "\n",
    "    def costFunctionReg(theta, X, y ,Lambda):\n",
    "\n",
    "        #Regularization and Computes Gradient\n",
    "    \n",
    "        m=len(y)\n",
    "        y=y[:,np.newaxis]\n",
    "        pred = sigmoid(X @ theta) # @ - matrix multiplication \n",
    "        error = (-y * np.log(pred)) - ((1-y)*np.log(1-pred))\n",
    "        cost = 1/m * sum(error)\n",
    "        regCost= cost + Lambda/(2*m) * sum(theta**2)\n",
    "        \n",
    "        # compute gradient ( Gradient Descent )\n",
    "        j_0= 1/m * (X.transpose() @ (pred - y))[0]\n",
    "        j_1 = 1/m * (X.transpose() @ (pred - y))[1:] + (Lambda/m)* theta[1:]\n",
    "        grad= np.vstack((j_0[:,np.newaxis],j_1)) # returns j_0 and j_1\n",
    "    \n",
    "        return regCost[0], grad\n",
    "    \n",
    "    initial_theta = np.zeros((X.shape[1], 1))\n",
    "\n",
    "    # Set regularization parameter lambda to 1\n",
    "    Lambda = 1\n",
    "\n",
    "    #Compute and display initial cost and gradient for regularized logistic regression\n",
    "    cost, grad=costFunctionReg(initial_theta, X, y, Lambda)\n",
    "    \n",
    "    \n",
    "    # Genereating Noise\n",
    "    def noisevector( scale, Length ):\n",
    "\n",
    "        r1 = np.random.uniform(0, 1, Length)#standard normal distribution\n",
    "        n1 = np.linalg.norm( r1, 1 )#get the norm of this random vector\n",
    "        r2 = r1 / n1#the norm of r2 is 1\n",
    "        normn = np.random.gamma( Length, scale, 1 )#Generate the norm of noise according to gamma distribution\n",
    "        res = r2 * normn#get the result noise vector\n",
    "        return res\n",
    "\n",
    "\n",
    "    def gradientDescent(X,y,theta,alpha,num_iters,Lambda):\n",
    "\n",
    "        # Generate theta (weights)\n",
    "    \n",
    "        m=len(y)\n",
    "        J_history =[]\n",
    "    \n",
    "        for i in range(num_iters):\n",
    "            cost, grad = costFunctionReg(theta,X,y,Lambda)\n",
    "            theta = theta - (alpha * grad) \n",
    "            J_history.append(cost)\n",
    "    \n",
    "        return theta , J_history\n",
    "    \n",
    "    theta , J_history = gradientDescent(X,y,initial_theta,1,900,0.2)\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates distributed diff. private noise values for single weight using secret sharing and S trick\n",
    "\n",
    "def calculate(n,weight_i): # n - number of parties/clients , weight_i - ith weight\n",
    "    \n",
    "    data = pd.read_csv(\"logReg.csv\")  \n",
    "    X=data.iloc[:,:-1].values\n",
    "    \n",
    "    \n",
    "    foo=[]\n",
    "    s = [[foo for i in range(n)] for j in range(n)]\n",
    "    t=[]\n",
    "    noiseS = [[t for i in range(n)] for j in range(n)]\n",
    "    nv=[]\n",
    "    noise = [[nv for i in range(n)] for j in range(n)]\n",
    "    \n",
    "    # Parties calculate shares and DP noise and send it to others\n",
    "    for i in range(n):\n",
    "    \n",
    "        theta = logisticRegression()\n",
    "        x=theta[weight_i] \n",
    "        Lambda=1\n",
    "        epsilon=1 # diff private loss parameter\n",
    "        scale = 2/( len(X) * Lambda * epsilon)\n",
    "        \n",
    "        w=[]\n",
    "        r=[]     \n",
    "    \n",
    "        # generate random values for secret sharing\n",
    "        for j in range(n-1):          \n",
    "            r.append(random.randint(1,100))\n",
    "           \n",
    "        \n",
    "        #generate random values for S trick\n",
    "        \n",
    "        for j in range(n):          \n",
    "            s[i][j]=random.randint(1,1000)\n",
    "           \n",
    "        sum=0\n",
    "        count=0\n",
    "        #secret sharing\n",
    "        for j in range(n):\n",
    "            if (j==0):\n",
    "                for k in range(n-1):\n",
    "                    sum += r[k]\n",
    "                w.append(x-sum)\n",
    "    \n",
    "            else:\n",
    "                w.append(r[j-1])\n",
    "              \n",
    "            pickle.dump( w[j]+s[i][j], open( \"weight\"+str(i)+str(j)+\".p\", \"wb\" ) ) \n",
    "            \n",
    "         \n",
    "        #generate random values for S trick\n",
    "        \n",
    "        for j in range(n):          \n",
    "            noiseS[i][j]=random.randint(1,1000)\n",
    "            \n",
    "                    \n",
    "        #generate DP noise vector (Gamma - Gamma) \n",
    "        \n",
    "        for j in range(n):\n",
    "            a=np.random.gamma(1/n,scale, 1)\n",
    "            b=np.random.gamma(1/n,scale, 1)\n",
    "            noise[i][j]= b-a\n",
    "            pickle.dump( noise[i][j]+noiseS[i][j], open( \"noise\"+str(i)+str(j)+\".p\", \"wb\" ) ) \n",
    "\n",
    "\n",
    "            \n",
    "#----------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "    #Parties calculate their share to be sent to server\n",
    "    for i in range(n):\n",
    "        \n",
    "        sum=0      \n",
    "        for j in range(n):        \n",
    "            sum = sum + pickle.load( open( \"weight\"+str(j)+str(i)+\".p\", \"rb\" ) ) + pickle.load( open( \"noise\"+str(j)+str(i)+\".p\", \"rb\" ) )- noiseS[i][j]-s[i][j]\n",
    "        \n",
    "        pickle.dump( sum, open( \"ro\"+str(i)+\".p\", \"wb\" ) )  \n",
    "        \n",
    "#----------------------------------------------------------------------------------------------------------------#    \n",
    "    #Server calculates weighted mean\n",
    "    \n",
    "    sum=0      \n",
    "    for i in range(n):\n",
    "        sum += pickle.load( open( \"ro\"+str(i)+\".p\", \"rb\" ) )\n",
    "   \n",
    "    print(sum/n)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.51938677]\n"
     ]
    }
   ],
   "source": [
    "calculate(2,1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
